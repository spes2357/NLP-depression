{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_NLP_Project_version2_with_K-fold_Cross_Validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spes2357/NLP-project/blob/main/LSTM_NLP_Project_version2_with_K_fold_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_FHzeH8pFqx",
        "outputId": "03d2e178-1468-4bc1-a274-6bfad6b605c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHifi-2u9F7P"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "import time\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvfypBo_adC"
      },
      "source": [
        "def getTextFromFiles(df, data_path, depression, limit):\n",
        "    \"\"\"Return Data Frame \"\"\"\n",
        "\n",
        "    for file in os.listdir(data_path)[:limit]:\n",
        "        with open(data_path + \"/\" + file, 'r', encoding=\"ISO-8859-1\") as file1:\n",
        "            file1 = file1.read()\n",
        "            df = df.append({'text': file1, 'depression': int(depression)}, ignore_index=True)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLgl-2iYDNp7"
      },
      "source": [
        "data_path_d = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_depression\"\n",
        "data_path_nd = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_non_depression\"\n",
        "df = pd.DataFrame(columns=['text', 'depression'])\n",
        "df = getTextFromFiles(df, data_path_d, 1, 500)\n",
        "df = getTextFromFiles(df, data_path_nd, 0, 500)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAZVt0fGw5qs"
      },
      "source": [
        "\n",
        "def RNN(model):\n",
        "\n",
        "  model.add(tf.keras.layers.Embedding(max_words,50,input_length=max_len))\n",
        "  model.add(tf.keras.layers.LSTM(64))\n",
        "  model.add(tf.keras.layers.Dense(256))\n",
        "  model.add(tf.keras.layers.Activation('relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "  model.build(input_shape=sequences_matrix.shape)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ6XcsoGt5LR"
      },
      "source": [
        "5-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJnGrWbstu8t",
        "outputId": "b9ecd7be-7023-40c2-8ad3-c4a04a342ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Input Data\n",
        "X = df['text'].to_numpy()\n",
        "Y = df['depression'].to_numpy().astype(float)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "batch_size = 4\n",
        "no_epochs = 10\n",
        "verbosity = 0\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "# inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "# targets = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "inputs = X\n",
        "targets = Y\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Data format matching\n",
        "  tok = Tokenizer(num_words=max_words)\n",
        "  tok.fit_on_texts(inputs[train])\n",
        "  sequences = tok.texts_to_sequences(inputs[train])\n",
        "  sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model = RNN(model)\n",
        "  # model.summary()\n",
        "  model.build(input_shape=sequences_matrix.shape)\n",
        "  # Compile the model\n",
        "  model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "  \n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model.fit(sequences_matrix, targets[train], batch_size=batch_size, epochs=no_epochs, verbose=verbosity)\n",
        "  \n",
        "  # Data format matching\n",
        "  test_sequences = tok.texts_to_sequences(inputs[test])\n",
        "  test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "  \n",
        "\n",
        "  # Generate generalization metrics\n",
        "  # accr = model.evaluate(test_sequences_matrix,Y_test)\n",
        "  # print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "  scores = model.evaluate(test_sequences_matrix, targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 1: loss of 0.8021815419197083; accuracy of 88.49999904632568%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 2: loss of 2.107591390609741; accuracy of 81.49999976158142%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 3: loss of 1.1366838216781616; accuracy of 85.00000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 4: loss of 1.4161094427108765; accuracy of 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 5: loss of 1.394975185394287; accuracy of 81.99999928474426%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.8021815419197083 - Accuracy: 88.49999904632568%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 2.107591390609741 - Accuracy: 81.49999976158142%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 1.1366838216781616 - Accuracy: 85.00000238418579%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 1.4161094427108765 - Accuracy: 87.99999952316284%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.394975185394287 - Accuracy: 81.99999928474426%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 85.0 (+- 2.915475824757554)\n",
            "> Loss: 1.371508276462555\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgFsymIRQmwv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}