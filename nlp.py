

# -*- coding: utf-8 -*-
"""CS5783 Project001
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/16d4HKi3na4TfZWN6nIY3TKbq5dBBpOB2
"""
# non depression dataset은 training 시킬 필요가 없다. 어차피 depression dataset에 없는 단어가 곧 non-depression 환자의 데이터 셋이 될거 같다.
# 추가로
import nltk
from nltk.stem import PorterStemmer
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# import seaborn as sns
import time
import re
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
import random
import os

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')


# dowmload and update
# from google.colab import drive
# drive.mount('/content/drive')

# trainging dataset 읽기
def getTextFromFiles(df, data_path, depression, limit):
    """Return Data Frame """

    for file in os.listdir(data_path)[:limit]:
        with open(data_path + "/" + file, 'r', encoding="ISO-8859-1") as file1:
            file1 = file1.read()
            df = df.append({'text': file1, 'depression': int(depression)}, ignore_index=True)

    return df

# test dataset 읽기
def getTextFromFiles_Test(df_test, data_path, limit):
    """Return Data Frame """

    for file in os.listdir(data_path)[:limit]:
        with open(data_path + "/" + file, 'r', encoding="ISO-8859-1") as file1:
            file1 = file1.read()
            df_test = df_test.append({'text': file1}, ignore_index=True)

    return df_test

# 데이터 전처리 과정
def dataPreprocessingForX(df, columnName1):
  #   읽어들인 데이터를 모두 소문자화 시킨다
  df[columnName1] = df[columnName1].map(lambda text: text.lower())
  # 단어 단위로 나눈다(word tokenzing)
  df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))
  # stoptwords를 이용하여 불필요한 단어 제거
  stop_words = set(nltk.corpus.stopwords.words('english'))
  df[columnName1] = df[columnName1].map(lambda tokens: [w for w in tokens if not w in stop_words])
  # regular expression을 이용한 특수문자 제거
  df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))
  df[columnName1] = df[columnName1].map(lambda text: re.sub('[^A-Za-z]+', ' ', text))
  # word stemmer를 톨해 단어를 원문화 시킨다.
  df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))
  ps = PorterStemmer()
  print(df[columnName1][0])
  df[columnName1] = df[columnName1].map(lambda text: [ps.stem(i) for i in text])
  df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))

# 파일이 제대로 읽혔는지 확인하는 function
def checkfilesCounts(data_path):
    print(len(os.listdir(data_path)))

# 개인 pc에서 실행시
data_path_d = "reddit_depression"
data_path_nd = "reddit_non_depression"
data_path_d_test = "reddit_depression_testset"
# collab에서 실행시
# data_path_d = "/content/drive/My Drive/NLP Team/code/reddit_depression"
# data_path_nd = "/content/drive/My Drive/NLP Team/code/reddit_non_depression"

checkfilesCounts(data_path_d)
checkfilesCounts(data_path_nd)
#
# # 데이터 전처리
df = pd.DataFrame(columns=['text', 'depression'])
df = getTextFromFiles(df, data_path_d, 1, 100)
# # 이시점까진 우울증 글만 추가
df = getTextFromFiles(df, data_path_nd, 0, 100)
dataPreprocessingForX(df, 'text')

df['depression'] = df['depression'].astype('int32')



# Countvectorizer : scikit-learn에서 Naive Bayes 분류기를 사용하기 전에 일단 자연어(텍스트)로 이루어진 문서들을 1과 0 밖에 모르는 컴퓨터가 이해할 수 있는 형식으로 변환해야 할 거다. feature extraction, 어휘(특성) 추출 과정이라 볼 수 있다.

# test_dataset에서 각 원문의 단어가 몇 번 나왔는지 확인 및 추출
count_vectorizer = CountVectorizer(ngram_range=(1,1), min_df=50)
#  fit_transform : fit과 transform을 합쳐놓은 문법
# fit : it() 메소드를 호출해서 학습 데이터 세트에 등장하는 어휘를 가르쳐놓아야 한다.
#  .transform()는 문자열 목록을 가져와 미리 학습해놓은 사전을 기반으로 어휘의 빈도를 세주는 거다.
counts = count_vectorizer.fit_transform(df['text'].values)
print(count_vectorizer.get_feature_names())

# print(counts.toarray())
tmp1 = counts.toarray()

scva = count_vectorizer.get_feature_names()
scvb = tmp1.sum(axis=0)

np.savetxt("csv1.csv",scva,delimiter=",", fmt="%s")
np.savetxt("csv2.csv",scvb,delimiter=",")


# test_dataset에서 각 원문의 단어가 몇 번 나왔는지 확인 및 추출
dump1 =np.array(counts)
df_dump1 = pd.DataFrame(columns=['text', "count"])
temp = dict()

df_test = pd.DataFrame(columns=['text'])

df_test = getTextFromFiles_Test(df_test, data_path_d_test,40)
# print(df_test.shape)
dataPreprocessingForX(df_test, 'text')
# print(df_test.shape)




counts = count_vectorizer.fit_transform(df['text'].tolist())


print(count_vectorizer.get_feature_names())
# print(counts.toarray())
tmp1 = counts.toarray()
print(tmp1.sum(axis=0))
scva = count_vectorizer.get_feature_names()
scvb = tmp1.sum(axis=0)

np.savetxt("csv23.csv",scva,delimiter=",", fmt="%s")
np.savetxt("csv24.csv",scvb,delimiter=",")

# dump1 = count_vectorizer.get_feature_names()
dump2 = counts.toarray()

# print(counts)
classifier = MultinomialNB()
targets = df['depression'].values
classifier.fit(counts, targets)



print(type(df_test))
example_counts = count_vectorizer.transform(df_test['text'].tolist())
predictions = classifier.predict(example_counts)

print(predictions)



tfidf_vectorizer = TfidfTransformer().fit(counts)
tfidf = tfidf_vectorizer.transform(counts)
classifier = MultinomialNB()
targets = df['depression'].values
classifier.fit(counts, targets)


example_counts = count_vectorizer.transform(df_test['text'].tolist())
example_tfidf = tfidf_vectorizer.transform(example_counts)
predictions_tfidf = classifier.predict(example_tfidf)
print(predictions_tfidf)





















# Wordcloud
# def makeWorldCloud():
#   depression_words = ''.join(list(df['text']))
#   depression_wordclod = WordCloud(width = 512,height = 512).generate(depression_words)
#   plt.figure(figsize = (10, 8), facecolor = 'k')
#   plt.imshow(depression_wordclod)
#   plt.axis('off')
#   plt.tight_layout(pad = 0)
#   plt.show()
# makeWorldCloud()

