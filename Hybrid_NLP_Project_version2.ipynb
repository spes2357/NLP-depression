{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hybrid_NLP_Project_version2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VYpDgOH4owdL"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianp0513/NLP-project/blob/main/Hybrid_NLP_Project_version2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkHUuou2n3Lt",
        "outputId": "ae2af6d3-c799-482d-8961-aca1a49150fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbloUVI6akO0"
      },
      "source": [
        "# data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHifi-2u9F7P",
        "outputId": "4ccdb13a-ae08-4de5-97d0-77fb3230bb4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "%matplotlib inline\n",
        "import time\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import random\n",
        "import os\n",
        "from sklearn import metrics \n",
        "from scipy.spatial.distance import cdist \n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "  \n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import OrderedDict\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import os\n",
        "plt.style.use('ggplot')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvfypBo_adC"
      },
      "source": [
        "def getTextFromFiles(df, data_path, depression, limit):\n",
        "    \"\"\"Return Data Frame \"\"\"\n",
        "\n",
        "    for file in os.listdir(data_path)[:limit]:\n",
        "        with open(data_path + \"/\" + file, 'r', encoding=\"ISO-8859-1\") as file1:\n",
        "            file1 = file1.read()\n",
        "            df = df.append({'text': file1, 'depression': int(depression)}, ignore_index=True)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRdvEWsDImo6"
      },
      "source": [
        "def dataPreprocessingForX(df, columnName1):\n",
        "    df[columnName1] = df[columnName1].map(lambda text: text.lower())\n",
        "    df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    df[columnName1] = df[columnName1].map(lambda tokens: [w for w in tokens if not w in stop_words])\n",
        "    df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "    df[columnName1] = df[columnName1].map(lambda text: re.sub('[^A-Za-z]+', ' ', text))\n",
        "    df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df[columnName1] = df[columnName1].map(lambda text: [lemmatizer.lemmatize(i) for i in text])\n",
        "    df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "\n",
        "def dataPreprocessingForY(df, columnName2):\n",
        "    df[columnName2] = df[columnName2].astype('int32')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7jNGPzy_w8g"
      },
      "source": [
        "data_path_d = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_depression\"\n",
        "data_path_nd = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_non_depression\"\n",
        "df = pd.DataFrame(columns=['text', 'depression'])\n",
        "df = getTextFromFiles(df, data_path_d, 1, 1000)\n",
        "df = getTextFromFiles(df, data_path_nd, 0, 500)\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5mv7cVlvN7q"
      },
      "source": [
        "dataPreprocessingForX(df, 'text')\n",
        "dataPreprocessingForY(df, 'depression')"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLgl-2iYDNp7"
      },
      "source": [
        "X = df['text'].to_numpy()\n",
        "Y = df['depression'].to_numpy()\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.30)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(X_train)\n",
        "sequences = tok.texts_to_sequences(X_train)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "test_sequences = tok.texts_to_sequences(X_test)\n",
        "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "\n",
        "\n",
        "# classifier.fit(counts, Y_train)\n",
        "# test_counts = count_vectorizer.transform(X_test)\n",
        "# predictions = classifier.predict(test_counts)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyw1U8NnJ5i2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypupi9sYap6i"
      },
      "source": [
        "# K mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-CljAjutz6H"
      },
      "source": [
        "\n",
        "def kmean(X_test_kmean):\n",
        "  init_centroid = 'k-means++'\n",
        "  y_kmList = []\n",
        "  distortions = [] \n",
        "  inertias = [] \n",
        "  mapping1 = {} \n",
        "  mapping2 = {}\n",
        "  K = range(1,10)\n",
        "  # X_train_kmean =  test_sequences_matrix\n",
        "  # X_train_kmean = counts.toarray()\n",
        "  for k in K:\n",
        "      #Building and fitting the model \n",
        "      kmeanModel = KMeans(n_clusters=k, init = init_centroid, random_state=0)\n",
        "      y_km = kmeanModel.fit(Xtest_kmean)  \n",
        "        \n",
        "      distortions.append(sum(np.min(cdist(X_test_kmean, kmeanModel.cluster_centers_, \n",
        "                        'euclidean'),axis=1)) / X_test_kmean.shape[0]) \n",
        "      inertias.append(kmeanModel.inertia_) \n",
        "\n",
        "      y_kmList.append(y_km.labels_)\n",
        "\n",
        "      mapping1[k] = sum(np.min(cdist(X_test_kmean, kmeanModel.cluster_centers_, \n",
        "                  'euclidean'),axis=1)) / X_test_kmean.shape[0] \n",
        "      mapping2[k] = kmeanModel.inertia_ \n",
        "\n",
        "\n",
        "  plt.plot(K, distortions, 'bx-') \n",
        "  plt.xlabel('Values of K') \n",
        "  plt.ylabel('Distortion') \n",
        "  plt.title('The Elbow Method using Distortion') \n",
        "  plt.show() \n",
        "\n",
        "  # for key,val in mapping1.items(): \n",
        "  #   print(str(key)+' : '+str(val))\n",
        "  \n",
        "  # plt.plot(K, inertias, 'bx-')\n",
        "  # plt.xlabel('Values of K')\n",
        "  # plt.ylabel('Inertia')\n",
        "  # plt.title('The Elbow Method using Inertia') \n",
        "  # plt.show() \n",
        "\n",
        "  # for key,val in mapping2.items(): \n",
        "  #   print(str(key)+' : '+str(val))\n",
        "\n",
        "\n",
        "  return y_kmList\n",
        "# print(np.shape(y_km))\n",
        "# plt.scatter(X[y_km==0, 0], X[y_km==0,1],c='lightgreen',marker='s',s=510,label='cluster1')\n",
        "# plt.scatter(X[y_km==1, 0], X[y_km==1,1],c='orange',marker='o',s=50,label='cluster2')\n",
        "# plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],c='red',marker='*',s=50,label='center')\n",
        "\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSp-kZQC2VjB"
      },
      "source": [
        "def kmeanClusterResult(y_kmList, X):\n",
        "  zero_list = []\n",
        "  one_list = []\n",
        "  two_list = []\n",
        "\n",
        "  for idx,iteration in enumerate(y_kmList[1]):\n",
        "    if iteration == 1:\n",
        "      one_list.append(idx)\n",
        "    elif iteration == 2:\n",
        "      two_list.append(idx)\n",
        "    else:\n",
        "      zero_list.append(idx)\n",
        "  # print(zero_list)\n",
        "  # print(one_list)\n",
        "  # print(two_list)\n",
        "  # print(X.shape)\n",
        "  cluster_zero = []\n",
        "  cluster_one = []\n",
        "  cluster_two = []\n",
        "  for index in zero_list:\n",
        "    sentence = X[index]\n",
        "    cluster_zero.append(sentence)\n",
        "    \n",
        "  # print(cluster_zero)\n",
        "  for index in one_list:\n",
        "    sentence = X[index]\n",
        "    cluster_one.append(sentence)\n",
        "\n",
        "  for index in two_list:\n",
        "    sentence = X[index]\n",
        "    cluster_two.append(sentence)\n",
        "\n",
        "  return zero_list, one_list"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y959wFxHfmnO"
      },
      "source": [
        "# Gaussian Mixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzRHIbbtelwM"
      },
      "source": [
        "def gmm(X_test_gmm):\n",
        "  y_gmmList = []\n",
        "\n",
        "  # X =  sequences_matrix\n",
        "  #Building and fitting the model \n",
        "  gmm = GaussianMixture(n_components = 2)\n",
        "  gmm.fit(X_test_gmm)\n",
        "  y_gmm = gmm.predict(X_test_gmm)\n",
        "  y_gmmList.append(y_gmm)\n",
        "\n",
        "  return y_gmmList\n",
        "\n",
        "def gmmClusterResult(y_gmmList, X):\n",
        "  zero_list = []\n",
        "  one_list = []\n",
        "  two_list = []\n",
        "\n",
        "  for idx,iteration in enumerate(y_gmmList[0]):\n",
        "    if iteration == 1:\n",
        "      one_list.append(idx)\n",
        "    elif iteration == 2:\n",
        "      two_list.append(idx)\n",
        "    else:\n",
        "      zero_list.append(idx)\n",
        "  # print(zero_list)\n",
        "  # print(one_list)\n",
        "  # print(two_list)\n",
        "  # print(X.shape)\n",
        "  cluster_zero = []\n",
        "  cluster_one = []\n",
        "  cluster_two = []\n",
        "  for index in zero_list:\n",
        "    sentence = X[index]\n",
        "    cluster_zero.append(sentence)\n",
        "    \n",
        "  # print(cluster_zero)\n",
        "  for index in one_list:\n",
        "    sentence = X[index]\n",
        "    cluster_one.append(sentence)\n",
        "\n",
        "  for index in two_list:\n",
        "    sentence = X[index]\n",
        "    cluster_two.append(sentence)\n",
        "\n",
        "  return zero_list, one_list"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aO2izT2vwL4"
      },
      "source": [
        "# print(sequences_matrix)\n",
        "# print(y_kmList)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95fRgH5C51nR"
      },
      "source": [
        "def makeWorldCloud(llist):\n",
        "  depression_words = ''.join(llist)\n",
        "  depression_wordclod = WordCloud(width = 512,height = 512).generate(depression_words)\n",
        "  plt.figure(figsize = (10, 8), facecolor = 'k')\n",
        "  plt.imshow(depression_wordclod)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()\n",
        "# makeWorldCloud(cluster_zero)\n",
        "# makeWorldCloud(cluster_one)\n",
        "# makeWorldCloud(cluster_two)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsU616HpJmnq"
      },
      "source": [
        "# y_kmList = kmean(sequences_matrix)\n",
        "# zero_list, one_list = kmeanClusterResult(y_kmList, X)\n",
        "y_gmmList = gmm(sequences_matrix)\n",
        "zero_list, one_list = gmmClusterResult(y_gmmList, X)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbtW5b2BauhJ"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78qCg4MFa0po",
        "outputId": "9427b53a-1bea-4eb0-80e5-d3ed46e01743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "# tok = Tokenizer(num_words=max_words)\n",
        "# tok.fit_on_texts(X_train)\n",
        "# sequences = tok.texts_to_sequences(X_train)\n",
        "# sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "# test_sequences = tok.texts_to_sequences(X_test)\n",
        "# test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "\n",
        "sequences_zeroClust = tok.texts_to_sequences(X[zero_list])\n",
        "sequences_matrix_zeroClust = sequence.pad_sequences(sequences_zeroClust,maxlen=max_len)\n",
        "sequences_oneClust = tok.texts_to_sequences(X[one_list])\n",
        "sequences_matrix_oneClust = sequence.pad_sequences(sequences_oneClust,maxlen=max_len)\n",
        "\n",
        "test_sequences_matrix = sequences_matrix_zeroClust\n",
        "Y_test_LSTM = Y[zero_list]\n",
        "\n",
        "# test_sequences_matrix = sequences_matrix_oneClust\n",
        "# Y_test = Y[one_list]\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_words,50,input_length=max_len))\n",
        "model.add(tf.keras.layers.LSTM(64))\n",
        "model.add(tf.keras.layers.Dense(256))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "\n",
        "# model.build(input_shape=sequences_matrix.shape)\n",
        "# model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "model.fit(sequences_matrix.astype(float), Y_train.astype(float), batch_size=4,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "accr = model.evaluate(test_sequences_matrix, Y_test_LSTM.astype(float))\n",
        "\n",
        "print(\" LSTM\")\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "210/210 [==============================] - 2s 11ms/step - loss: 0.6431 - accuracy: 0.6464 - val_loss: 0.5523 - val_accuracy: 0.6762\n",
            "Epoch 2/10\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.3991 - accuracy: 0.8357 - val_loss: 0.3099 - val_accuracy: 0.8667\n",
            "Epoch 3/10\n",
            "210/210 [==============================] - 2s 9ms/step - loss: 0.1717 - accuracy: 0.9381 - val_loss: 0.4034 - val_accuracy: 0.8333\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.1846 - accuracy: 0.9286\n",
            " LSTM\n",
            "Test set\n",
            "  Loss: 0.185\n",
            "  Accuracy: 0.929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtD7vQsWN5tJ"
      },
      "source": [
        ""
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYpDgOH4owdL"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwiNGScnozow",
        "outputId": "4b4e99b2-78d1-4803-f041-b3bc01bf8d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier = MultinomialNB()\n",
        "# count_vectorizer = CountVectorizer(ngram_range = ngram_range, min_df = min_df)\n",
        "# counts = count_vectorizer.fit_transform(X_train)\n",
        "# featnames = count_vectorizer.get_feature_names()\n",
        "# classifier.fit(counts, Y_train)\n",
        "\n",
        "ngram_range = (1,5)\n",
        "min_df = 0.1\n",
        "count_vectorizer = CountVectorizer(ngram_range = ngram_range, min_df = min_df)\n",
        "counts = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# featnames = count_vectorizer.get_feature_names()\n",
        "classifier.fit(counts, Y_train)\n",
        "print(X_test.shape)\n",
        "test_counts = count_vectorizer.transform(X_test)\n",
        "# X[zero_list]\n",
        "# y_kmList = kmean(test_counts.toarray())\n",
        "predictions = classifier.predict(test_counts)\n",
        "print(\"Accuracy: \",(predictions == Y_test).mean())\n",
        "# y_kmList = kmean(counts.toarray())\n",
        "# zero_list, one_list = kmeanClusterResult(y_kmList, X)\n",
        "\n",
        "y_gmmList = gmm(test_counts.toarray())\n",
        "zero_list, one_list = gmmClusterResult(y_gmmList, X)\n",
        "\n",
        "print(zero_list)\n",
        "print(one_list)\n",
        "\n",
        "\n",
        "test_counts = count_vectorizer.transform(X[zero_list])\n",
        "# predictions = classifier.predict(test_counts[zero_list])\n",
        "predictions = classifier.predict(test_counts)\n",
        "\n",
        "# Y_test = Y[zero_list]\n",
        "\n",
        "print(predictions)\n",
        "# Y[one_list]\n",
        "print(\"Accuracy: \",(predictions == Y[zero_list]).mean())\n",
        "\n",
        "test_counts = count_vectorizer.transform(X[one_list])\n",
        "# predictions = classifier.predict(test_counts[zero_list])\n",
        "predictions = classifier.predict(test_counts)\n",
        "# Y_test = Y[one_list]\n",
        "print(predictions)\n",
        "# Y[one_list]\n",
        "print(\"Accuracy: \",(predictions == Y[one_list]).mean())\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(450,)\n",
            "Accuracy:  0.8711111111111111\n",
            "[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 124, 125, 126, 129, 132, 135, 136, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 256, 257, 258, 259, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 342, 344, 345, 346, 347, 349, 351, 352, 353, 354, 355, 356, 357, 358, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 383, 384, 385, 387, 389, 392, 394, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 411, 412, 413, 414, 416, 417, 418, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 441, 442, 444, 445, 446, 447, 448]\n",
            "[4, 14, 24, 32, 43, 44, 47, 57, 61, 73, 75, 79, 93, 96, 120, 123, 127, 128, 130, 131, 133, 134, 137, 142, 157, 170, 172, 178, 188, 199, 212, 217, 228, 239, 241, 242, 255, 260, 274, 302, 321, 327, 338, 343, 348, 350, 359, 370, 381, 382, 386, 388, 390, 391, 393, 395, 409, 410, 415, 419, 440, 443, 449]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Accuracy:  0.9354005167958657\n",
            "[1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 0]\n",
            "Accuracy:  0.8571428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}