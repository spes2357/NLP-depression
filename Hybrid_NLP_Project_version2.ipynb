{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hybrid_NLP_Project_version2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianp0513/NLP-project/blob/main/Hybrid_NLP_Project_version2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkHUuou2n3Lt",
        "outputId": "daf51739-8385-441b-b4d6-5668e0475640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbloUVI6akO0"
      },
      "source": [
        "# data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHifi-2u9F7P",
        "outputId": "5277eb15-7573-4842-9e49-fda7e5614049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "%matplotlib inline\n",
        "import time\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import random\n",
        "import os\n",
        "from sklearn import metrics \n",
        "from scipy.spatial.distance import cdist \n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import EarlyStopping\n",
        "  \n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import OrderedDict\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import os\n",
        "plt.style.use('ggplot')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvfypBo_adC"
      },
      "source": [
        "def getTextFromFiles(df, data_path, depression, limit):\n",
        "    \"\"\"Return Data Frame \"\"\"\n",
        "\n",
        "    for file in os.listdir(data_path)[:limit]:\n",
        "        with open(data_path + \"/\" + file, 'r', encoding=\"ISO-8859-1\") as file1:\n",
        "            file1 = file1.read()\n",
        "            df = df.append({'text': file1, 'depression': int(depression)}, ignore_index=True)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRdvEWsDImo6"
      },
      "source": [
        "def dataPreprocessingForX(df, columnName1):\n",
        "    df[columnName1] = df[columnName1].map(lambda text: text.lower())\n",
        "    df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    df[columnName1] = df[columnName1].map(lambda tokens: [w for w in tokens if not w in stop_words])\n",
        "    df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "    df[columnName1] = df[columnName1].map(lambda text: re.sub('[^A-Za-z]+', ' ', text))\n",
        "    df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df[columnName1] = df[columnName1].map(lambda text: [lemmatizer.lemmatize(i) for i in text])\n",
        "    df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "\n",
        "def dataPreprocessingForY(df, columnName2):\n",
        "    df[columnName2] = df[columnName2].astype('int32')"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7jNGPzy_w8g"
      },
      "source": [
        "data_path_d = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_depression\"\n",
        "data_path_nd = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_non_depression\"\n",
        "df = pd.DataFrame(columns=['text', 'depression'])\n",
        "df = getTextFromFiles(df, data_path_d, 1, 1000)\n",
        "df = getTextFromFiles(df, data_path_nd, 0, 500)\n"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5mv7cVlvN7q"
      },
      "source": [
        "dataPreprocessingForX(df, 'text')\n",
        "dataPreprocessingForY(df, 'depression')"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLgl-2iYDNp7"
      },
      "source": [
        "X = df['text'].to_numpy()\n",
        "Y = df['depression'].to_numpy()\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.30)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(X_train)\n",
        "sequences = tok.texts_to_sequences(X_train)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "test_sequences = tok.texts_to_sequences(X_test)\n",
        "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "\n",
        "\n",
        "# classifier.fit(counts, Y_train)\n",
        "# test_counts = count_vectorizer.transform(X_test)\n",
        "# predictions = classifier.predict(test_counts)"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyw1U8NnJ5i2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y959wFxHfmnO"
      },
      "source": [
        "# Gaussian Mixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzRHIbbtelwM"
      },
      "source": [
        "def gmm(X_test_gmm):\n",
        "  y_gmmList = []\n",
        "\n",
        "  # X =  sequences_matrix\n",
        "  #Building and fitting the model \n",
        "  gmm = GaussianMixture(n_components = 2)\n",
        "  gmm.fit(X_test_gmm)\n",
        "  y_gmm = gmm.predict(X_test_gmm)\n",
        "  y_gmmList.append(y_gmm)\n",
        "  print(y_gmm.shape)\n",
        "  return y_gmmList\n",
        "\n",
        "def gmmClusterResult(y_gmmList, X):\n",
        "  zero_list = []\n",
        "  one_list = []\n",
        "  two_list = []\n",
        "\n",
        "  for idx,iteration in enumerate(y_gmmList[0]):\n",
        "    if iteration == 1:\n",
        "      one_list.append(idx)\n",
        "    elif iteration == 2:\n",
        "      two_list.append(idx)\n",
        "    else:\n",
        "      zero_list.append(idx)\n",
        "  # print(zero_list)\n",
        "  # print(one_list)\n",
        "  # print(two_list)\n",
        "  # print(X.shape)\n",
        "  cluster_zero = []\n",
        "  cluster_one = []\n",
        "  cluster_two = []\n",
        "  for index in zero_list:\n",
        "    sentence = X[index]\n",
        "    cluster_zero.append(sentence)\n",
        "    \n",
        "  # print(cluster_zero)\n",
        "  for index in one_list:\n",
        "    sentence = X[index]\n",
        "    cluster_one.append(sentence)\n",
        "\n",
        "  for index in two_list:\n",
        "    sentence = X[index]\n",
        "    cluster_two.append(sentence)\n",
        "\n",
        "  return zero_list, one_list"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aO2izT2vwL4"
      },
      "source": [
        "# print(sequences_matrix)\n",
        "# print(y_kmList)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95fRgH5C51nR"
      },
      "source": [
        "def makeWorldCloud(llist):\n",
        "  depression_words = ''.join(llist)\n",
        "  depression_wordclod = WordCloud(width = 512,height = 512).generate(depression_words)\n",
        "  plt.figure(figsize = (10, 8), facecolor = 'k')\n",
        "  plt.imshow(depression_wordclod)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()\n",
        "# makeWorldCloud(cluster_zero)\n",
        "# makeWorldCloud(cluster_one)\n",
        "# makeWorldCloud(cluster_two)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUfWiVmj8gEL",
        "outputId": "faa10119-85cd-4046-fa0e-4d56d32c77c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(test_sequences_matrix.shape)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(450, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsU616HpJmnq",
        "outputId": "414a8b00-87f8-41f0-e3c0-074fb43ce926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_gmmList_train = gmm(sequences_matrix)\n",
        "zero_list_train, one_list_train = gmmClusterResult(y_gmmList_train, X_train)\n",
        "y_gmmList_test = gmm(test_sequences_matrix)\n",
        "zero_list_test, one_list_test = gmmClusterResult(y_gmmList_test, X_test)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1050,)\n",
            "(450,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB3Ifkmy9ZBg"
      },
      "source": [
        ""
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbtW5b2BauhJ"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78qCg4MFa0po",
        "outputId": "d378361d-8093-4d03-9c54-ca4910e3d888",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "sequences_zeroClust = tok.texts_to_sequences(X_train[zero_list_train])\n",
        "sequences_matrix_zeroClust = sequence.pad_sequences(sequences_zeroClust,maxlen=max_len)\n",
        "sequences_oneClust = tok.texts_to_sequences(X_train[one_list_train])\n",
        "sequences_matrix_oneClust = sequence.pad_sequences(sequences_oneClust,maxlen=max_len)\n",
        "\n",
        "sequences_zeroClust = tok.texts_to_sequences(X_test[zero_list_test])\n",
        "sequences_matrix_zeroClust_test = sequence.pad_sequences(sequences_zeroClust,maxlen=max_len)\n",
        "sequences_oneClust = tok.texts_to_sequences(X_test[one_list_test])\n",
        "sequences_matrix_oneClust_test = sequence.pad_sequences(sequences_oneClust,maxlen=max_len)\n",
        "# Y_test_LSTM = Y_test[zero_list]\n",
        "\n",
        "# test_sequences_matrix = sequences_matrix_oneClust\n",
        "# Y_test = Y[one_list]\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_words,50,input_length=max_len))\n",
        "model.add(tf.keras.layers.LSTM(64))\n",
        "model.add(tf.keras.layers.Dense(256))\n",
        "model.add(tf.keras.layers.Activation('relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "\n",
        "# model.build(input_shape=sequences_matrix.shape)\n",
        "# model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
        "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "# model.fit(sequences_matrix.astype(float), Y_train.astype(float), batch_size=4,epochs=10,\n",
        "#           validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "\n",
        "# accr = model.evaluate(test_sequences_matrix, Y_test.astype(float))\n",
        "\n",
        "# print(\" LSTM\")\n",
        "# print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "model.fit(sequences_matrix_zeroClust.astype(float), Y_train[zero_list_train].astype(float), batch_size=4,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "\n",
        "accr = model.evaluate(sequences_matrix_zeroClust_test, Y_test[zero_list_test].astype(float))\n",
        "\n",
        "print(\" LSTM\")\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "model.fit(sequences_matrix_oneClust.astype(float), Y_train[one_list_train].astype(float), batch_size=4,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "\n",
        "\n",
        "accr = model.evaluate(sequences_matrix_oneClust_test, Y_test[one_list_test].astype(float))\n",
        "\n",
        "print(\" LSTM\")\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "72/72 [==============================] - 1s 14ms/step - loss: 0.6915 - accuracy: 0.5521 - val_loss: 0.6909 - val_accuracy: 0.5342\n",
            "Epoch 2/10\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 0.6842 - accuracy: 0.5521 - val_loss: 0.6808 - val_accuracy: 0.5479\n",
            "Epoch 3/10\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 0.5958 - accuracy: 0.7257 - val_loss: 0.4438 - val_accuracy: 0.8630\n",
            "Epoch 4/10\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 0.2439 - accuracy: 0.9097 - val_loss: 0.3194 - val_accuracy: 0.8356\n",
            "Epoch 5/10\n",
            "72/72 [==============================] - 1s 8ms/step - loss: 0.1789 - accuracy: 0.9826 - val_loss: 0.5045 - val_accuracy: 0.8493\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.6112 - accuracy: 0.6775\n",
            " LSTM\n",
            "Test set\n",
            "  Loss: 0.611\n",
            "  Accuracy: 0.678\n",
            "Epoch 1/10\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.3333 - accuracy: 0.8802 - val_loss: 0.3250 - val_accuracy: 0.8478\n",
            "Epoch 2/10\n",
            "138/138 [==============================] - 1s 9ms/step - loss: 0.1749 - accuracy: 0.9401 - val_loss: 0.3317 - val_accuracy: 0.8478\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.5078 - accuracy: 0.8112\n",
            " LSTM\n",
            "Test set\n",
            "  Loss: 0.508\n",
            "  Accuracy: 0.811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWRJUPE593qd",
        "outputId": "9908cc0f-1909-47c6-9af4-2a04279fa40a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y_test[one_list]"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
              "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    }
  ]
}