{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMM_NLP_Project_version1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianp0513/NLP-project/blob/main/GMM_NLP_Project_version1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkHUuou2n3Lt",
        "outputId": "8e17b69f-6255-4af4-ab85-67c408c48ad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHifi-2u9F7P"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "%matplotlib inline\n",
        "import time\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import random\n",
        "import os\n",
        "from sklearn import metrics \n",
        "from scipy.spatial.distance import cdist \n",
        "from sklearn.mixture import GaussianMixture "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvfypBo_adC"
      },
      "source": [
        "def getTextFromFiles(df, data_path, depression, limit):\n",
        "    \"\"\"Return Data Frame \"\"\"\n",
        "\n",
        "    for file in os.listdir(data_path)[:limit]:\n",
        "        with open(data_path + \"/\" + file, 'r', encoding=\"ISO-8859-1\") as file1:\n",
        "            file1 = file1.read()\n",
        "            df = df.append({'text': file1, 'depression': int(depression)}, ignore_index=True)\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7jNGPzy_w8g"
      },
      "source": [
        "data_path_d = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_depression\"\n",
        "data_path_nd = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_non_depression\"\n",
        "df = pd.DataFrame(columns=['text', 'depression'])\n",
        "df = getTextFromFiles(df, data_path_d, 1, 500)\n",
        "df = getTextFromFiles(df, data_path_nd, 0, 500)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLgl-2iYDNp7"
      },
      "source": [
        "X = df['text'].to_numpy()\n",
        "Y = df['depression'].to_numpy()\n",
        "# X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(X)\n",
        "sequences = tok.texts_to_sequences(X)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-CljAjutz6H",
        "outputId": "2dd4eea3-42e5-446a-aaa6-da3f2209ab35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "init_centroid = 'k-means++'\n",
        "y_gmmList = []\n",
        "distortions = [] \n",
        "inertias = [] \n",
        "mapping1 = {} \n",
        "mapping2 = {}\n",
        "K = range(1,10)\n",
        "X =  sequences_matrix\n",
        "\n",
        "\n",
        "#Building and fitting the model \n",
        "gmm = GaussianMixture(n_components = 2)\n",
        "gmm.fit(X)\n",
        "y_gmm = gmm.predict(X)\n",
        "y_gmmList.append(y_gmm)\n",
        "\n",
        "print(gmm.lower_bound_)\n",
        "print(gmm.n_iter_)\n",
        "\n",
        "\n",
        "# print(np.shape(y_km))\n",
        "# plt.scatter(X[y_km==0, 0], X[y_km==0,1],c='lightgreen',marker='s',s=510,label='cluster1')\n",
        "# plt.scatter(X[y_km==1, 0], X[y_km==1,1],c='orange',marker='o',s=50,label='cluster2')\n",
        "# plt.scatter(km.cluster_centers_[:,0],km.cluster_centers_[:,1],c='red',marker='*',s=50,label='center')\n",
        "\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-800.3208444480379\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktyZBZa7dSVr",
        "outputId": "f70722da-8db4-43ec-8c68-0f7ad2a63e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_gmm)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0\n",
            " 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1\n",
            " 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 1 0 1\n",
            " 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 1\n",
            " 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1\n",
            " 0 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0\n",
            " 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0\n",
            " 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0\n",
            " 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
            " 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
            " 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1 0\n",
            " 0 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 1 0 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 1 1 1\n",
            " 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1\n",
            " 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
            " 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
            " 1 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 0 0 1\n",
            " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1\n",
            " 0 0 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1\n",
            " 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aO2izT2vwL4"
      },
      "source": [
        "# print(sequences_matrix)\n",
        "# print(len(y_km))\n",
        "\n",
        "def gmmClusterResult(y_gmmList, X):\n",
        "  zero_list = []\n",
        "  one_list = []\n",
        "  two_list = []\n",
        "\n",
        "  for idx,iteration in enumerate(y_gmmList[0]):\n",
        "    if iteration == 1:\n",
        "      one_list.append(idx)\n",
        "    elif iteration == 2:\n",
        "      two_list.append(idx)\n",
        "    else:\n",
        "      zero_list.append(idx)\n",
        "  # print(zero_list)\n",
        "  # print(one_list)\n",
        "  # print(two_list)\n",
        "  # print(X.shape)\n",
        "  cluster_zero = []\n",
        "  cluster_one = []\n",
        "  cluster_two = []\n",
        "  for index in zero_list:\n",
        "    sentence = X[index]\n",
        "    cluster_zero.append(sentence)\n",
        "    \n",
        "  # print(cluster_zero)\n",
        "  for index in one_list:\n",
        "    sentence = X[index]\n",
        "    cluster_one.append(sentence)\n",
        "\n",
        "  for index in two_list:\n",
        "    sentence = X[index]\n",
        "    cluster_two.append(sentence)\n",
        "\n",
        "  return zero_list, one_list\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b0MYw_ZbftC",
        "outputId": "dceebae1-86f1-4059-ebd8-ace4db4809de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "zero_list, one_list = gmmClusterResult(y_gmmList, X)\n",
        "\n",
        "one_list\n",
        "# print(\"Accuracy: \",(Y[one_list] == Y_test).mean())\n",
        "print(Y[zero_list])\n",
        "print(Y[one_list])\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95fRgH5C51nR"
      },
      "source": [
        "def makeWorldCloud(llist):\n",
        "  depression_words = ''.join(llist)\n",
        "  depression_wordclod = WordCloud(width = 512,height = 512).generate(depression_words)\n",
        "  plt.figure(figsize = (10, 8), facecolor = 'k')\n",
        "  plt.imshow(depression_wordclod)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout(pad = 0)\n",
        "  plt.show()\n",
        "# makeWorldCloud(cluster_o)\n",
        "# makeWorldCloud(cluster_l)"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}