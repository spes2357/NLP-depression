{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_NLP_Project_version4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianp0513/NLP-project/blob/main/LSTM_NLP_Project_version4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5O6gNGKodX3"
      },
      "source": [
        "# LSTM with custom text preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl3EL67pTvc2"
      },
      "source": [
        "Failure note:\n",
        "\n",
        "I tried to use Countvecter for keras, but it did not work.\n",
        "Keras seems to work with own builtin text transform function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXmofp6Qkh1o",
        "outputId": "1c46797b-07af-43ce-e5e5-261e73c605a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHifi-2u9F7P",
        "outputId": "bbd5fd8b-ec43-4776-a205-80c5343d9f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "# import seaborn as sns\n",
        "import time\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import random\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnr1PI1imZX_"
      },
      "source": [
        "class LSTMversionFour:\n",
        "\n",
        "    def __init__(self, numberOfDFiles, numberOfNDFiles, data_path_d=\"reddit_depression\",\n",
        "                 data_path_nd=\"reddit_non_depression\"):\n",
        "\n",
        "        self.data_path_d = data_path_d\n",
        "        self.data_path_nd = data_path_nd\n",
        "        self.data_path_d_test = \"reddit_depression_testset\"\n",
        "        self.df = pd.DataFrame(columns=['text', 'depression'])\n",
        "        self.numberOfDFiles = numberOfDFiles\n",
        "        self.numberOfNDFiles = numberOfNDFiles\n",
        "        self.depressionClass = 1\n",
        "        self.nonDepressionClass = 0\n",
        "        self.classifier = MultinomialNB()\n",
        "        self.counts = 0\n",
        "        self.test_counts= 0\n",
        "        self.featnames = 0\n",
        "        self.length_of_maximum_document = 0\n",
        "\n",
        "    def preprocessing(self):\n",
        "\n",
        "        self.checkfilesCounts(self.data_path_d)\n",
        "        self.checkfilesCounts(self.data_path_nd)\n",
        "        self.df = self.getTextFromFiles(self.df, self.data_path_d, self.depressionClass, self.numberOfDFiles)\n",
        "        self.df = self.getTextFromFiles(self.df, self.data_path_nd, self.nonDepressionClass, self.numberOfNDFiles)\n",
        "        self.dataPreprocessingForX(self.df, 'text')\n",
        "        self.dataPreprocessingForY(self.df, 'depression')\n",
        "        X = self.df['text'].values\n",
        "        Y = self.df['depression'].values\n",
        "        indices = np.arange(len(X))\n",
        "        X_train, X_test, Y_train, Y_test, idx_train, idx_test = train_test_split(X, Y, indices, test_size=0.15)\n",
        "        return X_train, X_test, Y_train, Y_test, idx_train, idx_test\n",
        "\n",
        "\n",
        "    def getTextFromFiles(self, df, data_path, zeroOrOne , limit):\n",
        "        \"\"\"Return Data Frame \"\"\"\n",
        "\n",
        "        for file in os.listdir(data_path)[:limit]:\n",
        "            with open(data_path + \"/\" + file, 'r', encoding=\"ISO-8859-1\") as file1:\n",
        "                file1 = file1.read()\n",
        "                df = df.append({'text': file1, 'depression': int(zeroOrOne)}, ignore_index=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    def dataPreprocessingForX(self, df, columnName1):\n",
        "\n",
        "        df[columnName1] = df[columnName1].map(lambda text: text.lower())\n",
        "        df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "        df[columnName1] = df[columnName1].map(lambda tokens: [w for w in tokens if not w in stop_words])\n",
        "        df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "        df[columnName1] = df[columnName1].map(lambda text: re.sub('[^A-Za-z]+', ' ', text))\n",
        "        df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        df[columnName1] = df[columnName1].map(lambda text: [lemmatizer.lemmatize(i) for i in text])\n",
        "        self.length_of_maximum_document = df[columnName1].map(lambda listOfwords: len(listOfwords)).max()\n",
        "        df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "        # lenght_of_maximum_document\n",
        "\n",
        "    def dataPreprocessingForY(self, df, columnName2):\n",
        "\n",
        "        df[columnName2] = df[columnName2].astype('int32')\n",
        "\n",
        "\n",
        "    def checkfilesCounts(self, data_path):\n",
        "        print(\"files Counts in \"+ data_path +\": \", len(os.listdir(data_path)))\n",
        "\n",
        "    def fitandPredict(self, X_train, Y_train, X_test):\n",
        "\n",
        " \n",
        "        # predictions = self.classifier.predict(self.test_counts)\n",
        "        # print(self.counts.shape)\n",
        "        # sequences_matrix = self.counts.toarray()\n",
        "        # sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "        # test_sequences_matrix = self.test_counts.toarray()\n",
        "\n",
        "        # sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "        # 'input_dim' = the vocab size that we will choose. In other words it is the number of unique words in the vocab.\n",
        "        # 'output_dim' = the number of dimensions we wish to embed into. Each word will be represented by a vector of this much dimensions.\n",
        "        # 'input_length' = lenght of the maximum document. which is stored in maxlen variable in our case.\n",
        "        # input_dim = len(self.featnames)\n",
        "        # output_dim = 10\n",
        "        # input_length = self.length_of_maximum_document\n",
        "\n",
        "        max_words = 1000\n",
        "        max_len = 150\n",
        "\n",
        "        tokenizer = Tokenizer(num_words=max_words)\n",
        "        tokenizer.fit_on_texts(X_train)\n",
        "        sequences = tokenizer.texts_to_sequences(X_train)\n",
        "        sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "        test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "        test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "\n",
        "        # print(\"input_dim\", len(self.featnames))\n",
        "        # print(\"output_dim\",10)\n",
        "        # print(\"input_length\", self.length_of_maximum_document)\n",
        "        # print(\"sequences_matrix.shape\", sequences_matrix.shape)\n",
        "        # print(\"sequences_matrix.shape\", test_sequences_matrix.shape)\n",
        "\n",
        "        model = tf.keras.models.Sequential()\n",
        "        model.add(tf.keras.layers.Embedding(input_dim = max_words, output_dim = 8, input_length= max_len))\n",
        "        # model.add(tf.keras.layers.Embedding(max_words,50))\n",
        "        model.add(tf.keras.layers.LSTM(64))\n",
        "        model.add(tf.keras.layers.Dropout(0.2))\n",
        "        model.add(tf.keras.layers.Dense(256))\n",
        "        # model.add(tf.keras.layers.Activation('relu'))\n",
        "        # model.add(tf.keras.layers.Dropout(0.2))\n",
        "        # model.add(tf.keras.layers.Dense(80))\n",
        "        model.add(tf.keras.layers.Activation('relu'))\n",
        "        model.add(tf.keras.layers.Dropout(0.2))\n",
        "        model.add(tf.keras.layers.Dense(1))\n",
        "        model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "\n",
        "        model.build(input_shape=sequences_matrix.shape)\n",
        "        \n",
        "        # model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "        model.compile(loss='binary_crossentropy',optimizer= opt,metrics=['accuracy'])\n",
        "        model.summary()\n",
        "        model.fit(sequences_matrix, Y_train.astype(float), batch_size=4,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
        "        accr = model.evaluate(test_sequences_matrix,Y_test.astype(float))\n",
        "        print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "        # print(predictions)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8KCEPgQsD6B",
        "outputId": "e8bf7d7e-b8b1-4709-a831-9a9850f9e34a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# obj = LSTMversionFour(numberOfDFiles =1000, numberOfNDFiles=500, data_path_d=\"reddit_depression\",\n",
        "#                      data_path_nd=\"reddit_non_depression\")\n",
        "data_path_d = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_depression\"\n",
        "data_path_nd = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_non_depression\"\n",
        "obj = LSTMversionFour(numberOfDFiles =500, numberOfNDFiles=500, data_path_d=data_path_d,\n",
        "                     data_path_nd=data_path_nd)\n",
        "X_train, X_test, Y_train, Y_test, idx_train, idx_test = obj.preprocessing()\n",
        "predictions = obj.fitandPredict( X_train, Y_train, X_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "files Counts in /content/drive/My Drive/NLP Team/code/kerasData/reddit_depression:  1293\n",
            "files Counts in /content/drive/My Drive/NLP Team/code/kerasData/reddit_non_depression:  548\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 150, 8)            8000      \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 64)                18688     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 43,585\n",
            "Trainable params: 43,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "170/170 [==============================] - 2s 11ms/step - loss: 0.6931 - accuracy: 0.4941 - val_loss: 0.6924 - val_accuracy: 0.5471\n",
            "Epoch 2/10\n",
            "170/170 [==============================] - 1s 8ms/step - loss: 0.6917 - accuracy: 0.5515 - val_loss: 0.6907 - val_accuracy: 0.6059\n",
            "Epoch 3/10\n",
            "170/170 [==============================] - 1s 8ms/step - loss: 0.6835 - accuracy: 0.6412 - val_loss: 0.6820 - val_accuracy: 0.5824\n",
            "Epoch 4/10\n",
            "170/170 [==============================] - 1s 8ms/step - loss: 0.5799 - accuracy: 0.7588 - val_loss: 0.3385 - val_accuracy: 0.9235\n",
            "Epoch 5/10\n",
            "170/170 [==============================] - 1s 8ms/step - loss: 0.3773 - accuracy: 0.8750 - val_loss: 0.3119 - val_accuracy: 0.8588\n",
            "Epoch 6/10\n",
            "170/170 [==============================] - 1s 9ms/step - loss: 0.2555 - accuracy: 0.9000 - val_loss: 0.3211 - val_accuracy: 0.9000\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3330 - accuracy: 0.8667\n",
            "Test set\n",
            "  Loss: 0.333\n",
            "  Accuracy: 0.867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgFsymIRQmwv"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}