{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_NLP_Project_version3_with_K-fold_Cross_Validation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianp0513/NLP-project/blob/main/LSTM_NLP_Project_version3_with_K_fold_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_FHzeH8pFqx",
        "outputId": "7231d2b0-0225-411f-8e52-113faf635752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHifi-2u9F7P",
        "outputId": "21dee8a0-b48b-4846-a706-3ff9241f2744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "import time\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import random\n",
        "import os\n",
        "import tensorflow as tf\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvfypBo_adC"
      },
      "source": [
        "def getTextFromFiles(df, data_path, depression, limit):\n",
        "    \"\"\"Return Data Frame \"\"\"\n",
        "\n",
        "    for file in os.listdir(data_path)[:limit]:\n",
        "        with open(data_path + \"/\" + file, 'r', encoding=\"ISO-8859-1\") as file1:\n",
        "            file1 = file1.read()\n",
        "            df = df.append({'text': file1, 'depression': int(depression)}, ignore_index=True)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0iJ0ao7gsLv"
      },
      "source": [
        "def dataPreprocessingForX(df, columnName1):\n",
        "    df[columnName1] = df[columnName1].map(lambda text: text.lower())\n",
        "    df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    df[columnName1] = df[columnName1].map(lambda tokens: [w for w in tokens if not w in stop_words])\n",
        "    df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "    df[columnName1] = df[columnName1].map(lambda text: re.sub('[^A-Za-z]+', ' ', text))\n",
        "    df[columnName1] = df[columnName1].map(lambda text: nltk.tokenize.word_tokenize(text))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df[columnName1] = df[columnName1].map(lambda text: [lemmatizer.lemmatize(i) for i in text])\n",
        "    df[columnName1] = df[columnName1].map(lambda text: ' '.join(text))\n",
        "\n",
        "def dataPreprocessingForY(df, columnName2):\n",
        "    df[columnName2] = df[columnName2].astype('int32')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLgl-2iYDNp7"
      },
      "source": [
        "data_path_d = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_depression\"\n",
        "data_path_nd = \"/content/drive/My Drive/NLP Team/code/kerasData/reddit_non_depression\"\n",
        "df = pd.DataFrame(columns=['text', 'depression'])\n",
        "df = getTextFromFiles(df, data_path_d, 1, 1000)\n",
        "df = getTextFromFiles(df, data_path_nd, 0, 500)\n",
        "dataPreprocessingForX(df, 'text')\n",
        "dataPreprocessingForY(df, 'depression')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAZVt0fGw5qs"
      },
      "source": [
        "\n",
        "def RNN(model):\n",
        "\n",
        "  model.add(tf.keras.layers.Embedding(input_dim = max_words, output_dim = 8, input_length= max_len))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64 , return_sequences=True)))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(80))\n",
        "  model.add(tf.keras.layers.Activation('relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "  # model.build(input_shape=sequences_matrix.shape)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytVd7-oAp-N9"
      },
      "source": [
        "def RNN2(model):\n",
        "\n",
        "  model.add(tf.keras.layers.Embedding(input_dim = max_words, output_dim = 10, input_length= max_len))\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True)))\n",
        "  model.add(tf.keras.layers.Dense(80))\n",
        "  model.add(tf.keras.layers.Activation('tanh'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "  # model.build(input_shape=sequences_matrix.shape)\n",
        "  # model.build(input_shape=sequences_matrix.shape)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCbcAkncrLWR"
      },
      "source": [
        " def CNN_LSTM(model): \n",
        "  model.add(tf.keras.layers.Embedding(max_words,50,input_length=max_len))\n",
        "  model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.LSTM(64))\n",
        "  # model.add(tf.keras.layers.Dense(256))\n",
        "  # model.add(tf.keras.layers.Activation('relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc0Ml95Ysk3V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d61PAFRJslis"
      },
      "source": [
        "# 2 BiLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ6XcsoGt5LR"
      },
      "source": [
        "5-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJnGrWbstu8t",
        "outputId": "c3117dab-eb56-43b8-c734-36300a202383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Input Data\n",
        "\n",
        "\n",
        "X = df['text'].to_numpy()\n",
        "Y = df['depression'].to_numpy().astype(float)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "batch_size = 4\n",
        "no_epochs = 10\n",
        "verbosity = 0\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "# inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "# targets = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "inputs = X\n",
        "targets = Y\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Data format matching\n",
        "  tok = Tokenizer(num_words=max_words)\n",
        "  tok.fit_on_texts(inputs[train])\n",
        "  sequences = tok.texts_to_sequences(inputs[train])\n",
        "  sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model = RNN(model)\n",
        "  # model.summary()\n",
        "  model.build(input_shape=sequences_matrix.shape)\n",
        "  # Compile the model\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  \n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model.fit(sequences_matrix, targets[train], batch_size=batch_size, epochs=no_epochs, verbose=verbosity)\n",
        "  \n",
        "  # Data format matching\n",
        "  test_sequences = tok.texts_to_sequences(inputs[test])\n",
        "  test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "  \n",
        "\n",
        "  # Generate generalization metrics\n",
        "  # accr = model.evaluate(test_sequences_matrix,Y_test)\n",
        "  # print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "  scores = model.evaluate(test_sequences_matrix, targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 1: loss of 0.39350879192352295; accuracy of 87.33333349227905%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 2: loss of 0.2682547867298126; accuracy of 89.66666460037231%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 3: loss of 0.2735030949115753; accuracy of 89.33333158493042%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 4: loss of 0.44815099239349365; accuracy of 89.66666460037231%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 5: loss of 0.29126405715942383; accuracy of 91.00000262260437%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.39350879192352295 - Accuracy: 87.33333349227905%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.2682547867298126 - Accuracy: 89.66666460037231%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.2735030949115753 - Accuracy: 89.33333158493042%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.44815099239349365 - Accuracy: 89.66666460037231%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.29126405715942383 - Accuracy: 91.00000262260437%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 89.3999993801117 (+- 1.18133683107112)\n",
            "> Loss: 0.3349363446235657\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyVSMnk8stm6"
      },
      "source": [
        "# BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgFsymIRQmwv",
        "outputId": "771a1134-0e44-446e-d47a-fea2d82bf66e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Input Data\n",
        "\n",
        "\n",
        "X = df['text'].to_numpy()\n",
        "Y = df['depression'].to_numpy().astype(float)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "batch_size = 4\n",
        "no_epochs = 10\n",
        "verbosity = 0\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "# inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "# targets = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "inputs = X\n",
        "targets = Y\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Data format matching\n",
        "  tok = Tokenizer(num_words=max_words)\n",
        "  tok.fit_on_texts(inputs[train])\n",
        "  sequences = tok.texts_to_sequences(inputs[train])\n",
        "  sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model = RNN2(model)\n",
        "  # model.summary()\n",
        "  model.build(input_shape=sequences_matrix.shape)\n",
        "  # Compile the model\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  \n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model.fit(sequences_matrix, targets[train], batch_size=batch_size, epochs=no_epochs, verbose=verbosity)\n",
        "  \n",
        "  # Data format matching\n",
        "  test_sequences = tok.texts_to_sequences(inputs[test])\n",
        "  test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "  \n",
        "\n",
        "  # Generate generalization metrics\n",
        "  # accr = model.evaluate(test_sequences_matrix,Y_test)\n",
        "  # print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "  scores = model.evaluate(test_sequences_matrix, targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 1: loss of 0.2703160047531128; accuracy of 89.87554907798767%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 2: loss of 0.34834665060043335; accuracy of 87.79111504554749%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 3: loss of 0.3335161805152893; accuracy of 89.73778486251831%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 4: loss of 0.33494457602500916; accuracy of 87.5889003276825%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 5: loss of 0.2337017059326172; accuracy of 90.7977819442749%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.2703160047531128 - Accuracy: 89.87554907798767%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.34834665060043335 - Accuracy: 87.79111504554749%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.3335161805152893 - Accuracy: 89.73778486251831%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.33494457602500916 - Accuracy: 87.5889003276825%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2337017059326172 - Accuracy: 90.7977819442749%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 89.15822625160217 (+- 1.2546202609015151)\n",
            "> Loss: 0.30416502356529235\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71DUdcI-sxNj"
      },
      "source": [
        "# CNN+LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D139gxrrSZp",
        "outputId": "d9ecae5f-3d9a-437e-d78c-b88d237f5e4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "# Input Data\n",
        "\n",
        "\n",
        "X = df['text'].to_numpy()\n",
        "Y = df['depression'].to_numpy().astype(float)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n",
        "batch_size = 4\n",
        "no_epochs = 10\n",
        "verbosity = 0\n",
        "\n",
        "num_folds = 5\n",
        "\n",
        "\n",
        "\n",
        "# Define per-fold score containers <-- these are new\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "# inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "# targets = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "inputs = X\n",
        "targets = Y\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Data format matching\n",
        "  tok = Tokenizer(num_words=max_words)\n",
        "  tok.fit_on_texts(inputs[train])\n",
        "  sequences = tok.texts_to_sequences(inputs[train])\n",
        "  sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model = CNN_LSTM(model)\n",
        "  # model.summary()\n",
        "  model.build(input_shape=sequences_matrix.shape)\n",
        "  # Compile the model\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  \n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model.fit(sequences_matrix, targets[train], batch_size=batch_size, epochs=no_epochs, verbose=verbosity)\n",
        "  \n",
        "  # Data format matching\n",
        "  test_sequences = tok.texts_to_sequences(inputs[test])\n",
        "  test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "  \n",
        "\n",
        "  # Generate generalization metrics\n",
        "  # accr = model.evaluate(test_sequences_matrix,Y_test)\n",
        "  # print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
        "\n",
        "  scores = model.evaluate(test_sequences_matrix, targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Score for fold 1: loss of 0.31100690364837646; accuracy of 88.99999856948853%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Score for fold 2: loss of 0.227126345038414; accuracy of 93.33333373069763%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Score for fold 3: loss of 0.495379239320755; accuracy of 86.66666746139526%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Score for fold 4: loss of 0.38000231981277466; accuracy of 86.33333444595337%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Score for fold 5: loss of 0.2107895463705063; accuracy of 92.33333468437195%\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.31100690364837646 - Accuracy: 88.99999856948853%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.227126345038414 - Accuracy: 93.33333373069763%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.495379239320755 - Accuracy: 86.66666746139526%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.38000231981277466 - Accuracy: 86.33333444595337%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.2107895463705063 - Accuracy: 92.33333468437195%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 89.53333377838135 (+- 2.864340156359882)\n",
            "> Loss: 0.3248608708381653\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}